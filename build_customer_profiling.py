"""
Iterate the link got from the json file generated by scrape_amazon_review.py to get all posts by each review
"""
import itertools
import json
from datetime import datetime
import emoji
from lexical_diversity import lex_div as ld
import textstat
import pickle

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

import ssl
import pandas as pd

first_date_available_list = ["June 1, 2021", "January 12, 2016", "February 14, 2019", "May 30, 2021",
                             "September 13, 2019", "September 24, 2021", "August 17, 2020", "February 15, 2011",
                             "February 1, 2012", "April 10, 2020", "October 11, 2017"]
first_date_available_list2 = ["May 19, 2020", "August 2, 2018", "October 18, 2022", "August 4, 2021"]
count_review = 0

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download('punkt')
nltk.download('stopwords')

stop_words = set(stopwords.words("english"))
punctuation = set(string.punctuation)
removing_words = stop_words | punctuation

review_length_list = []
count_keyword_list = []
word_diversity_measure_list = []
word_complexity_measure_list = []
whether_image_list = []
whether_emoji_list = []
timing_list = []
count_helpful_list = []


def read_json(json_address):
    with open(json_address, 'r') as f:
        # Load the contents of the file into a Python object
        data = json.load(f)
    flattened_list = list(itertools.chain(*data))
    return flattened_list


def get_count_review(review_list):
    global count_review
    count_review = len(review_list)


def get_body_length(review_list):
    # point 1
    for entry in review_list:
        review_length_list.append(len(entry["body"].split(" ")))


def get_keywords(review_list):
    # point 2
    keywords = []
    for entry in review_list:
        tokens = word_tokenize(entry["body"])
        keywords.append([token for token in tokens if not token.lower() in removing_words])
    return keywords


def get_keywords_number(keywords):
    # point 2
    for ks in keywords:
        count_keyword_list.append(len(ks))


def get_word_diversity(review_list):
    # point 3
    for entry in review_list:
        word_diversity_measure_list.append(ld.mtld(ld.flemmatize(entry["body"])))


def get_word_complexity(review_list):
    # point 4
    for entry in review_list:
        word_complexity_measure_list.append(textstat.automated_readability_index(entry["body"]))


def get_whether_image(review_list):
    # point 5
    for dictionary in review_list:
        whether_image_list.append(dictionary["image_usage"])


def get_whether_emoji(review_list):
    # point 6
    get_emoji = False
    for dictionary in review_list:
        value = dictionary["body"]
        for character in value:
            if character in emoji.EMOJI_DATA:
                get_emoji = True
                whether_emoji_list.append(1)
                break
        if get_emoji:
            get_emoji = False
            continue
        else:
            whether_emoji_list.append(0)


def get_timing(review_list):
    # point 8
    for dictionary in review_list:
        review_date = dictionary["written_date"]
        index = dictionary["product_index"]
        first_available_date = first_date_available_list[index]
        date_1 = datetime.strptime(review_date, '%B %d, %Y')
        date_2 = datetime.strptime(first_available_date, '%B %d, %Y')
        timing = date_1 - date_2
        timing_list.append(timing.days)


def get_count_helpful(review_list):
    # point 9
    for dictionary in review_list:
        count_helpful_list.append(dictionary["people_count"])


def buildup_dataset(review_list):
    get_count_review(review_list)
    print(f"There are {count_review} number of reviews")

    # point 1: the length of the review by words
    get_body_length(review_list)
    print(review_length_list)
    # print(len(review_length_list))

    # point 2: the count of keywords in the review, filter by NLTK package methods
    keywords = get_keywords(review_list)
    with open('key_word_list.pickle', 'wb') as file:
        pickle.dump(keywords, file)
    get_keywords_number(keywords)
    print(count_keyword_list)
    # print(len(count_keyword_list))


    # point 3: word diversity in the review
    get_word_diversity(review_list)
    print(word_diversity_measure_list)
    # print(len(word_diversity_measure_list))

    # point 4: word complexity in the review
    get_word_complexity(review_list)
    print(word_complexity_measure_list)
    # print(len(word_complexity_measure_list))

    # point 5: whether this review contains image
    get_whether_image(review_list)
    print(whether_image_list)
    # print(len(whether_image_list))

    # point 6: whether this review contains emoji
    get_whether_emoji(review_list)
    print(whether_emoji_list)
    # print(len(whether_emoji_list))

    # point 8: the day difference between first available date and the posted available date
    get_timing(review_list)
    print(timing_list)
    # print(len(timing_list))

    # point 9: the count of people who think this review is helpful
    get_count_helpful(review_list)
    print(count_helpful_list)
    # print(len(count_helpful_list))

    # point 10: the target variable: the divergent thinking score
    with open("creativity_measure_list.pickle", 'rb') as f:
        creativity_measure_list = pickle.load(f)
    print(creativity_measure_list)
    # print(len(creativity_measure_list))

    df = pd.DataFrame({"review_length": review_length_list, "number_of_keywords": count_keyword_list,
                       "word_diversity": word_diversity_measure_list, "word_complexity": word_complexity_measure_list,
                       "has_image": whether_image_list, "has_emoji": whether_emoji_list, "timing": timing_list,
                       "count_helpful": count_helpful_list, "creativity" : creativity_measure_list}
                       )
    df.to_csv('customer_behavior_prediction_dataframe.csv', index=False)


if __name__ == '__main__':
    json_address = "final_file.json"
    review_list = read_json(json_address)
    buildup_dataset(review_list)
